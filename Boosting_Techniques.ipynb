{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques | Assignment"
      ],
      "metadata": {
        "id": "ydpG523H0wl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners."
      ],
      "metadata": {
        "id": "zHfA9Iox0yDM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dec3b6e"
      },
      "source": [
        "**Boosting** is a powerful ensemble machine learning technique that combines multiple \"weak\" or \"base\" learners (models that perform slightly better than random guessing) into a single \"strong\" learner. The core idea is to iteratively improve the model's performance by focusing on the examples that previous weak learners misclassified.\n",
        "\n",
        "### How it improves weak learners:\n",
        "\n",
        "1.  **Sequential Learning:** Unlike bagging (e.g., Random Forest) where models are trained independently, boosting trains models sequentially. Each new weak learner is built to correct the errors of the previous ones.\n",
        "2.  **Weighted Data:** In each iteration, boosting algorithms assign higher weights to the data points that were misclassified by the previous weak learners. This forces the subsequent learners to pay more attention to these difficult examples.\n",
        "3.  **Weighted Voting/Averaging:** When making a final prediction, the predictions from all weak learners are combined, often with a weighted average or vote. More accurate weak learners (those that performed better on the data) are given higher influence in the final decision.\n",
        "\n",
        "By repeatedly identifying and addressing the weaknesses of previous models, boosting gradually transforms a collection of simple, not-so-good models into a highly accurate and robust predictive model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?"
      ],
      "metadata": {
        "id": "Sap11A2G04Li"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7f35309"
      },
      "source": [
        "Both AdaBoost and Gradient Boosting are sequential ensemble methods that combine multiple weak learners to form a strong learner. However, they differ fundamentally in how they train these weak learners and combine their predictions:\n",
        "\n",
        "### AdaBoost (Adaptive Boosting)\n",
        "\n",
        "*   **Focus:** AdaBoost focuses on **misclassified data points**. In each iteration, it adjusts the weights of the training instances, giving more weight to observations that were misclassified by the previous weak learner.\n",
        "*   **Weak Learner Training:** Each subsequent weak learner is trained on the re-weighted dataset, forcing it to pay more attention to the previously difficult examples.\n",
        "*   **Weak Learner Contribution:** Each weak learner gets a weight assigned to it based on its accuracy. More accurate learners get higher weights in the final ensemble.\n",
        "*   **Combination:** Predictions from all weak learners are combined through a weighted majority vote (for classification) or weighted sum (for regression), where learners with higher accuracy have more influence.\n",
        "*   **Error Minimization:** It primarily minimizes an exponential loss function.\n",
        "\n",
        "### Gradient Boosting\n",
        "\n",
        "*   **Focus:** Gradient Boosting focuses on **errors (residuals) of the previous predictor**. Instead of re-weighting data points, it trains subsequent weak learners to predict the residuals (the difference between the actual value and the prediction of the current ensemble).\n",
        "*   **Weak Learner Training:** Each new weak learner is trained to predict the \"negative gradient\" of the loss function with respect to the current ensemble's prediction. In simpler terms, it tries to correct the errors made by the sum of all previous models.\n",
        "*   **Weak Learner Contribution:** Each weak learner's prediction is multiplied by a small learning rate (shrinkage) before being added to the ensemble, which helps prevent overfitting.\n",
        "*   **Combination:** Predictions are combined by summing the predictions of all weak learners, each typically scaled by a learning rate. The final model is a sum of these individual models.\n",
        "*   **Error Minimization:** It is a more generalized algorithm that can optimize any differentiable loss function by iteratively moving down the gradient of the loss function.\n",
        "\n",
        "### Key Differences Summarized:\n",
        "\n",
        "| Feature             | AdaBoost                                    | Gradient Boosting                                    |\n",
        "| :------------------ | :------------------------------------------ | :--------------------------------------------------- |\n",
        "| **Error Correction**| Re-weights misclassified instances          | Fits new models to residuals (negative gradients)    |\n",
        "| **Loss Function**   | Typically exponential loss                  | Can optimize any differentiable loss function        |\n",
        "| **Weak Learner Output** | Predicts the target variable directly   | Predicts the residuals (errors)                      |\n",
        "| **Final Ensemble**  | Weighted sum/vote of weak learners          | Sum of weak learners' predictions (scaled by learning rate) |\n",
        "\n",
        "In essence, AdaBoost iteratively corrects its errors by focusing on difficult data points, while Gradient Boosting iteratively corrects its errors by training new models to predict the remaining error from the previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "gO6iOoB406n-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39e1e72"
      },
      "source": [
        "Regularization is a crucial aspect of XGBoost that helps prevent overfitting and improves the model's generalization capabilities. XGBoost incorporates two main types of regularization:\n",
        "\n",
        "1.  **L1 Regularization (Lasso Regression):**\n",
        "    *   Adds a penalty equal to the absolute value of the magnitude of coefficients. This encourages sparsity, meaning it can shrink some feature coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "2.  **L2 Regularization (Ridge Regression):**\n",
        "    *   Adds a penalty equal to the square of the magnitude of coefficients. This penalizes large coefficients, making the model less sensitive to individual data points and promoting smaller, more stable weights.\n",
        "\n",
        "### How Regularization Helps in XGBoost:\n",
        "\n",
        "*   **Prevents Overfitting:** By penalizing complex models (e.g., trees with too many leaves or large coefficient values), regularization discourages the model from memorizing the training data, leading to better performance on unseen data.\n",
        "*   **Controls Tree Complexity:** XGBoost specifically uses regularization terms in its objective function that penalize the number of leaves in a tree and the magnitude of the leaf weights. This prevents individual trees from becoming too complex and overfitting the training data.\n",
        "*   **Feature Selection:** L1 regularization can drive the weights of less important features to zero, effectively selecting the most relevant features and simplifying the model.\n",
        "*   **Smoother Models:** Regularization generally leads to smoother decision boundaries and more generalized models, reducing variance.\n",
        "\n",
        "In essence, regularization in XGBoost acts as a control mechanism, balancing the model's ability to fit the training data with its ability to generalize to new, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "iqP8lPKj06fm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49e16447"
      },
      "source": [
        "CatBoost is particularly efficient and effective for handling categorical features primarily due to its innovative approach to processing them. Unlike many other gradient boosting algorithms that require explicit one-hot encoding or other preprocessing of categorical features, CatBoost handles them internally.\n",
        "\n",
        "Here are the key reasons:\n",
        "\n",
        "1.  **Ordered Target Statistics (Ordered TS):** This is CatBoost's most distinctive feature for categorical data. Instead of computing the average target value for each category based on the entire dataset (which can lead to target leakage and overfitting), CatBoost uses a 'prior-based' approach. For each data point, it computes the target statistics (e.g., average target value) for a category using only the data points *before* the current one in a random permutation of the dataset. This prevents target leakage and results in more robust and generalized embeddings for categorical features.\n",
        "\n",
        "2.  **Permutation-driven Ordering:** CatBoost generates multiple random permutations of the dataset. For each permutation, it calculates the ordered target statistics. This helps to reduce bias and variance that might arise from a single ordering.\n",
        "\n",
        "3.  **Automatic Categorical Feature Combinations:** CatBoost can automatically combine different categorical features to create new, more informative features. For example, if you have 'City' and 'Product' as categorical features, CatBoost can create a new feature like 'City_and_Product' if it finds that this combination is predictive. This capability is especially powerful in datasets with many categorical features and complex interactions.\n",
        "\n",
        "4.  **Handling of Rare Categories:** The ordered target statistics approach naturally handles rare categories better than traditional one-hot encoding, which can lead to very sparse matrices. By incorporating priors and considering the order, CatBoost can derive meaningful representations even for categories with few occurrences.\n",
        "\n",
        "5.  **No Need for Extensive Preprocessing:** Because CatBoost handles categorical features internally, users don't need to manually perform one-hot encoding, label encoding, or other complex transformations. This simplifies the data preprocessing pipeline and reduces the chances of errors or target leakage.\n",
        "\n",
        "In summary, CatBoost's intelligent use of Ordered Target Statistics, permutation-driven ordering, automatic feature combination, and internal handling mechanisms make it highly efficient and performant for datasets rich in categorical information, often outperforming other algorithms in such scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?"
      ],
      "metadata": {
        "id": "wUenLbnk06Hr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c5fd0bc"
      },
      "source": [
        "Boosting techniques are often preferred over bagging methods in scenarios where high accuracy is paramount, and the dataset might have complex relationships or a need to focus on difficult-to-classify instances. Here are some real-world applications:\n",
        "\n",
        "1.  **Fraud Detection:** In financial institutions, boosting algorithms like XGBoost and LightGBM are highly effective for detecting fraudulent transactions. They can learn from rare fraud patterns and adjust to misclassified cases, leading to higher recall and precision in identifying fraudulent activities.\n",
        "\n",
        "2.  **Credit Scoring and Risk Assessment:** Banks and lending institutions use boosting for credit scoring, assessing the risk of loan defaults. Boosting models can capture intricate relationships between various financial indicators and borrower behavior, leading to more accurate risk predictions.\n",
        "\n",
        "3.  **Image and Object Recognition:** While deep learning has become dominant, boosting (especially Gradient Boosting) has been successfully applied in various image processing tasks, particularly for feature-based recognition. For instance, Haar cascades (built using AdaBoost) were historically used in real-time face detection.\n",
        "\n",
        "4.  **Medical Diagnosis:** Boosting can assist in diagnosing diseases by analyzing patient data, medical images, and laboratory results. Its ability to handle complex, often imbalanced, medical datasets helps in identifying subtle patterns indicative of a disease.\n",
        "\n",
        "5.  **Customer Churn Prediction:** Telecommunication companies, subscription services, and other businesses use boosting to predict which customers are likely to churn (cancel their service). By focusing on customers at risk, companies can implement targeted retention strategies.\n",
        "\n",
        "6.  **Ad Click-Through Rate (CTR) Prediction:** In online advertising, predicting whether a user will click on an ad is crucial. Boosting models are widely used to optimize ad placement by accurately predicting CTRs, improving advertising revenue.\n",
        "\n",
        "7.  **Recommendation Systems:** Boosting can enhance recommendation engines by modeling complex user preferences and item characteristics. While not as common as collaborative filtering or matrix factorization, it can be used to rank items for recommendation.\n",
        "\n",
        "8.  **Drug Discovery and Genomics:** In bioinformatics, boosting can be applied to classify molecules, predict drug efficacy, or identify genes associated with certain conditions by handling high-dimensional and complex biological data.\n",
        "\n",
        "**Why boosting is preferred in these cases:**\n",
        "\n",
        "*   **Higher Accuracy:** Boosting's iterative nature allows it to continuously improve performance by correcting errors of previous models, often leading to state-of-the-art accuracy on many tabular datasets.\n",
        "*   **Handles Complex Relationships:** It can model non-linear relationships and interactions between features more effectively than simpler models.\n",
        "*   **Focus on Hard Examples:** By giving more weight to misclassified instances (AdaBoost) or residuals (Gradient Boosting), it specifically targets and learns from the most challenging data points.\n",
        "*   **Robust to Outliers (with proper tuning):** While sensitive to noisy data, with proper regularization and hyperparameter tuning, boosting can be quite robust and often performs well even with some outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "##tasks.\n",
        "- Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "- Print the model accuracy\n",
        "\n",
        "###Datasets:\n",
        "- Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "- Use sklearn.datasets.fetch_california_housing() for regression"
      ],
      "metadata": {
        "id": "sv0jTLu3058a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e6a2453",
        "outputId": "5bff177a-88d5-4945-f9c9-886a86006df0"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "bcs_data = load_breast_cancer()\n",
        "X = bcs_data.data\n",
        "y = bcs_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "adaboost_model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy on Breast Cancer Dataset: {accuracy:.4f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer Dataset: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "- Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "- Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "4-IL-iRx05yb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e30804",
        "outputId": "1cb84895-b5c6-4073-8ef7-1555b11fbb28"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing_data = fetch_california_housing()\n",
        "X_housing = california_housing_data.data\n",
        "y_housing = california_housing_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train_housing, y_train_housing)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_housing = gbr_model.predict(X_test_housing)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test_housing, y_pred_housing)\n",
        "print(f\"Gradient Boosting Regressor R-squared score on California Housing Dataset: {r2:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score on California Housing Dataset: 0.7803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "- Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "- Tune the learning rate using GridSearchCV\n",
        "- Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "5GZifuV005lW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5d2fc84",
        "outputId": "d1ffb892-dd17-4f65-facf-e19435ad8d7d"
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# The Breast Cancer dataset (X, y) and split data (X_train, X_test, y_train, y_test) are already loaded from previous questions.\n",
        "# If not, uncomment and run the following lines:\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# bcs_data = load_breast_cancer()\n",
        "# X = bcs_data.data\n",
        "# y = bcs_data.target\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV - tuning learning_rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# We use a smaller cv value (e.g., 3) for quicker execution in an example.\n",
        "# For production, a higher cv value (e.g., 5 or 10) is recommended.\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set with the best model\n",
        "y_pred_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the best model\n",
        "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
        "print(f\"XGBoost Classifier Accuracy with best parameters: {accuracy_tuned:.4f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "Best parameters found: {'learning_rate': 0.05}\n",
            "XGBoost Classifier Accuracy with best parameters: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "- Train a CatBoost Classifier\n",
        "- Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "6XK002tg04_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# X_train, X_test, y_train, y_test are already loaded from previous questions (Breast Cancer dataset).\n",
        "\n",
        "# Initialize the CatBoost Classifier\n",
        "# Setting verbose=0 to suppress training output\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='Logloss', random_seed=42, verbose=0)\n",
        "\n",
        "# Train the model\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_catboost = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_catboost = accuracy_score(y_test, y_pred_catboost)\n",
        "print(f\"CatBoost Classifier Accuracy on Breast Cancer Dataset: {accuracy_catboost:.4f}\")\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_catboost)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "xEIcQ6p43epC",
        "outputId": "0796d5e4-3952-48bf-f92b-a06ead0caba8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CatBoost Classifier Accuracy on Breast Cancer Dataset: 0.9708\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "#The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
        "#Describe your step-by-step data science pipeline using boosting techniques:\n",
        "- Data preprocessing & handling missing/categorical values\n",
        "- Choice between AdaBoost, XGBoost, or CatBoost\n",
        "- Hyperparameter tuning strategy\n",
        "- Evaluation metrics you'd choose and why\n",
        "- How the business would benefit from your model"
      ],
      "metadata": {
        "id": "60Yv_ubk30dY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d556ea59"
      },
      "source": [
        "## Data Science Pipeline for Loan Default Prediction (FinTech Company)\n",
        "\n",
        "Given an imbalanced dataset with missing values, and both numeric and categorical features, here's a step-by-step data science pipeline using boosting techniques to predict loan default:\n",
        "\n",
        "### 1. Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "*   **Missing Values:**\n",
        "    *   **Numerical:** Median imputation (robust to outliers in financial data) as the primary approach. For larger proportions of missing data, KNN imputation could be explored.\n",
        "    *   **Categorical:** Mode imputation or creating a new `'Missing'` category if missingness is informative.\n",
        "*   **Categorical Features:**\n",
        "    *   **Low Cardinality:** One-Hot Encoding to avoid ordinal assumptions.\n",
        "    *   **High Cardinality:** Target Encoding (with smoothing/cross-validation to prevent leakage) or, ideally, leverage CatBoost's native handling.\n",
        "\n",
        "### 2. Choice of Boosting Algorithm: CatBoost (Recommended)\n",
        "\n",
        "**Recommendation: CatBoost**\n",
        "\n",
        "*   **Justification:** CatBoost is the most suitable due to its superior handling of mixed data types.\n",
        "    *   **Native Categorical Handling:** Employs Ordered Target Statistics to transform categorical features without explicit encoding, preventing target leakage and high dimensionality, especially crucial for loan datasets rich in demographic/behavioral categories.\n",
        "    *   **Built-in Missing Value Handling:** Can process missing values natively, reducing manual imputation effort.\n",
        "    *   **Robustness to Overfitting:** Its ordered boosting scheme and regularization make it less prone to overfitting, vital for imbalanced and critical financial predictions.\n",
        "\n",
        "*   **Why not others?**\n",
        "    *   **XGBoost:** A strong contender, but requires manual preprocessing of categorical features, which CatBoost handles more elegantly and robustly.\n",
        "    *   **AdaBoost:** Less ideal due to sensitivity to noisy data/outliers, and lack of native support for categorical features and missing values, requiring extensive and potentially error-prone preprocessing.\n",
        "\n",
        "### 3. Hyperparameter Tuning Strategy\n",
        "\n",
        "*   **Search Methods:** Start with **RandomizedSearchCV** for broad exploration, then refine with **GridSearchCV** in promising regions of the hyperparameter space.\n",
        "*   **Cross-Validation:** Use **Stratified K-Fold Cross-Validation** to ensure each fold maintains the same class distribution as the original imbalanced dataset, providing reliable performance estimates.\n",
        "*   **Imbalance Handling:** Incorporate **class weighting** (e.g., `scale_pos_weight` in XGBoost/CatBoost) during tuning to give more importance to the minority class (defaults). Consider sampling techniques (e.g., SMOTE) applied within cross-validation folds.\n",
        "\n",
        "### 4. Evaluation Metrics (and why Accuracy is misleading)\n",
        "\n",
        "Standard accuracy is misleading for imbalanced datasets because a model can achieve high accuracy by simply predicting the majority class, failing to identify the critical minority class (defaulters).\n",
        "\n",
        "Appropriate metrics:\n",
        "*   **Recall (Sensitivity):** Crucial for minimizing False Negatives (missed defaulters), reducing financial losses.\n",
        "*   **Precision:** Important for minimizing False Positives (incorrectly flagging non-defaulters), avoiding customer dissatisfaction.\n",
        "*   **F1-Score:** Balances Precision and Recall, providing a holistic measure of performance.\n",
        "*   **AUC-ROC (Area Under the Receiver Operating Characteristic Curve):** Evaluates the model's overall discriminative power across various thresholds, less sensitive to class imbalance.\n",
        "*   **AUC-PR (Area Under the Precision-Recall Curve):** Often preferred for highly imbalanced datasets as it focuses on the performance of the minority class, giving a more realistic view than AUC-ROC.\n",
        "\n",
        "### 5. Business Benefits\n",
        "\n",
        "Implementing this model provides significant tangible benefits for the FinTech company:\n",
        "\n",
        "*   **Reduced Financial Losses:** Proactively identifies high-risk loans, minimizing write-offs and bad debt provisions.\n",
        "*   **Improved Risk Assessment:** Offers granular, data-driven risk profiling, acting as an early warning system for potential defaults.\n",
        "*   **Enhanced Decision-Making:** Enables faster, more consistent, and objective lending decisions, reducing human bias.\n",
        "*   **Optimized Resource Allocation:** Directs collection efforts more efficiently and allows for targeted marketing of loan products.\n",
        "*   **Customized Products:** Insights facilitate the development of tailored loan products with differentiated terms, attracting a broader, creditworthy customer base.\n",
        "\n",
        "This pipeline empowers the FinTech company to move from reactive to proactive risk management, leading to greater financial stability and a competitive edge."
      ]
    }
  ]
}